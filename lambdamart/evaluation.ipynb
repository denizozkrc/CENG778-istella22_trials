{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Istella22 - LambdaMART evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "\n",
    "from rankeval.dataset import Dataset\n",
    "from rankeval.model import RTEnsemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a dataset using RankEval (https://github.com/hpclab/rankeval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LambdaMART\n",
    "#TEST_FILE=\"data/test.svm\"\n",
    "#MODEL_FILE=\"models/lambdamart.lgb\"\n",
    "\n",
    "# LambdaMART, MonoT5\n",
    "#TEST_FILE=\"data/test.monoT5.svm\"\n",
    "#MODEL_FILE=\"models/lambdamart.monoT5.lgb\"\n",
    "\n",
    "# LambdaMART, MonoT5, Title, Url and Text\n",
    "#TEST_FILE=\"data/test.monoT5.titleUrlText.svm\"\n",
    "#MODEL_FILE=\"models/lambdamart.monoT5.titleUrlText.lgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subset = Dataset.load(TEST_FILE, format=\"svmlight\", name=\"Istella-test\")\n",
    "\n",
    "print(\"Istella - Test Set\")\n",
    "print(\"Num. features: \", test_subset.n_features)\n",
    "print(\"Num. queries: \", test_subset.n_queries)\n",
    "print(\"Num. instances: \", test_subset.n_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the LightGBM (https://github.com/microsoft/LightGBM) LambdaMART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** using RankEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_lmart = RTEnsemble(MODEL_FILE, name=\"LGBM_lmart\", format=\"LightGBM\")\n",
    "\n",
    "print(\"Model statistics\")\n",
    "print(\"Num. Trees: \", lgbm_lmart.n_trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.metrics import Precision, Recall, NDCG, MRR, MAP\n",
    "\n",
    "mrr = MRR()\n",
    "ap = MAP()\n",
    "precision_1 = Precision(cutoff=1, threshold=1)\n",
    "precision_5 = Precision(cutoff=5, threshold=1)\n",
    "precision_10 = Precision(cutoff=10, threshold=1)\n",
    "\n",
    "recall_100 = Recall(cutoff=100)\n",
    "recall_1000 = Recall(cutoff=1000)\n",
    "\n",
    "ndcg_5 = NDCG(cutoff=5, no_relevant_results=0, implementation='exp')\n",
    "ndcg_10 = NDCG(cutoff=10, no_relevant_results=0, implementation='exp')\n",
    "ndcg_20 = NDCG(cutoff=20, no_relevant_results=0, implementation='exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.effectiveness import model_performance\n",
    "\n",
    "istella_model_perf = model_performance(\n",
    "    datasets=[test_subset], \n",
    "    models=[lgbm_lmart], \n",
    "    metrics=[mrr, ap,\n",
    "             precision_1, precision_5, precision_10,\n",
    "             recall_100, recall_1000,\n",
    "             ndcg_5, ndcg_10, ndcg_20]\n",
    "    )\n",
    "\n",
    "istella_model_perf.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *** using ir_measures (https://github.com/terrierteam/ir_measures/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lgbm_lmart.score(test_subset, detailed=False)\n",
    "print(y_pred[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_subset.get_query_sizes()[0:5])\n",
    "\n",
    "y = test_subset.y\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid = 0\n",
    "total_counter = 0\n",
    "\n",
    "run = []\n",
    "qrels = []\n",
    "\n",
    "while (qid < len(test_subset.get_query_sizes())):\n",
    "    doc_id = 0\n",
    "    while (doc_id < test_subset.get_query_sizes()[qid]):\n",
    "        run.append(ir_measures.ScoredDoc(str(qid), str(doc_id), float(y_pred[total_counter])))\n",
    "        qrels.append(ir_measures.Qrel(str(qid), str(doc_id), int(y[total_counter])))\n",
    "        doc_id += 1\n",
    "        total_counter += 1\n",
    "    qid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measures_dict = ir_measures.calc_aggregate([AP, RR, P(rel=1)@1, P(rel=1)@5, P(rel=1)@10, R@100, R@1000, nDCG(dcg='exp-log2')@5, nDCG(dcg='exp-log2')@10, nDCG(dcg='exp-log2')@20, Judged@10], qrels, run)\n",
    "#print(measures_dict)\n",
    "\n",
    "measures_df = pd.DataFrame.from_dict(measures_dict, orient='index')\n",
    "measures_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical significance analysis (using RankEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.statistical import statistical_significance\n",
    "\n",
    "# lmart vs lmart_monoT5\n",
    "MODEL_FILE_1=\"models/lambdamart.lgb\"\n",
    "MODEL_FILE_2=\"models/lambdamart_monoT5.lgb\"\n",
    "\n",
    "lmart1 = RTEnsemble(MODEL_FILE_1, name=\"LGBM_lmart_1\", format=\"LightGBM\")\n",
    "lmart2 = RTEnsemble(MODEL_FILE_2, name=\"LGBM_lmart_2\", format=\"LightGBM\")\n",
    "\n",
    "stat_sig = statistical_significance(datasets=[test_subset],\n",
    "                                    model_a=lmart1, model_b=lmart2, \n",
    "                                    metrics=[mrr, ap,\n",
    "                                             precision_1, precision_5, precision_10,\n",
    "                                             ndcg_10, ndcg_20],\n",
    "                                    n_perm=100000\n",
    "                                   )\n",
    "stat_sig.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rankeval.analysis.statistical import statistical_significance\n",
    "\n",
    "# lmart vs lmart_monoT5_titleUrlText\n",
    "MODEL_FILE_1=\"models/lambdamart.lgb\"\n",
    "MODEL_FILE_2=\"models/lambdamart_monoT5_titleUrlText.lgb\"\n",
    "\n",
    "lmart1 = RTEnsemble(MODEL_FILE_1, name=\"LGBM_lmart_1\", format=\"LightGBM\")\n",
    "lmart2 = RTEnsemble(MODEL_FILE_2, name=\"LGBM_lmart_2\", format=\"LightGBM\")\n",
    "\n",
    "stat_sig = statistical_significance(datasets=[test_subset],\n",
    "                                    model_a=lmart1, model_b=lmart2, \n",
    "                                    metrics=[mrr, ap,\n",
    "                                             precision_1, precision_5, precision_10,\n",
    "                                             ndcg_10, ndcg_20],\n",
    "                                    n_perm=100000\n",
    "                                   )\n",
    "stat_sig.to_dataframe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
